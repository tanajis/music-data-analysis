-----------------------------------------------
DATA LAKE
-----------------------------------------------
Read src data
1.Song data
2.Log data

Process data using spark
Transforms them to create five different tables listed below :


Fact Tables:
-----------------------------------------------

1.songplays - records in log data associated with song plays i.e. records with page NextSong
  Fields  -songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent


Dimension Tables
-----------------------------------------------
2.users - users in the app
  Fields - user_id, first_name, last_name, gender, level
3.songs - songs in music database
  Fields - song_id, title, artist_id, year, duration
4.artists - artists in music database
  Fields - artist_id, name, location, lattitude, longitude
5.time - timestamps of records in songplays broken down into specific units
  Fields - start_time, hour, day, week, month, year, weekday
-----------------------------------------------


Hadoop dir
music_data_analysis
  -data_lake
    -songs
    -logs



-----------------------
# install Dependencies :
pip install -r requirements.txt


#---------------------------------------------------------
Data warehouse
#---------------------------------------------------------

Fact Tables
-----------------------------------------------
songplays - records in event data associated with song plays.
Columns for the table:
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent


Dimension Tables
-----------------------------------------------
users:
user_id, first_name, last_name, gender, level

songs:
song_id, title, artist_id, year, duration

artists:
artist_id, name, location, lattitude, longitude

time
start_time, hour, day, week, month, year, weekday

#---------------------------------------------------------
